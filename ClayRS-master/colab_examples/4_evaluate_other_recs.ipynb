{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "4_evaluate_other_recs.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGfRczMXt-Tt"
   },
   "source": [
    "# Installation with pip\n",
    "Every dependency needed by the framework will be downloaded and installed automatically"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_z0TVVHmQ2sU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "22983974-5433-419c-c350-0002bf2e2b00"
   },
   "source": [
    "!pip install clayrs"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting clayrs\n",
      "  Downloading clayrs-0.4.0.tar.gz (225 kB)\n",
      "\u001B[K     |████████████████████████████████| 225 kB 20.1 MB/s \n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "    Preparing wheel metadata ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting distex~=0.7.1\n",
      "  Using cached distex-0.7.2-py3-none-any.whl (19 kB)\n",
      "Collecting transformers~=4.15.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.4 MB 53.1 MB/s \n",
      "\u001B[?25hCollecting pyaml~=21.10.1\n",
      "  Using cached pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
      "Collecting mysql-connector-python~=8.0.20\n",
      "  Downloading mysql_connector_python-8.0.33-cp38-cp38-manylinux1_x86_64.whl (27.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 27.4 MB 61.1 MB/s \n",
      "\u001B[?25hCollecting ekphrasis~=0.5.4\n",
      "  Using cached ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
      "Collecting matplotlib~=3.2.2\n",
      "  Downloading matplotlib-3.2.2-cp38-cp38-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 12.4 MB 90.9 MB/s \n",
      "\u001B[?25hCollecting scikit-learn==1.0.2\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 26.7 MB 1.3 MB/s \n",
      "\u001B[?25hCollecting nltk~=3.5\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.5 MB 50.1 MB/s \n",
      "\u001B[?25hCollecting spacy~=3.2.1\n",
      "  Downloading spacy-3.2.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001B[K     |████████████████████████████████| 6.2 MB 50.1 MB/s \n",
      "\u001B[?25hCollecting babelpy~=1.0.1\n",
      "  Using cached BabelPy-1.0.1.tar.gz (8.0 kB)\n",
      "Collecting mysql~=0.0.3\n",
      "  Using cached mysql-0.0.3-py3-none-any.whl (1.2 kB)\n",
      "Collecting numpy~=1.21.6\n",
      "  Downloading numpy-1.21.6-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 15.7 MB 49.7 MB/s \n",
      "\u001B[?25hCollecting pandas~=1.2.4\n",
      "  Downloading pandas-1.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 9.7 MB 80.3 MB/s \n",
      "\u001B[?25hCollecting whoosh~=2.7.4\n",
      "  Using cached Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Collecting sentence-transformers~=1.2.0\n",
      "  Using cached sentence-transformers-1.2.1.tar.gz (80 kB)\n",
      "Collecting scipy~=1.7.3\n",
      "  Downloading scipy-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 39.3 MB 1.2 MB/s \n",
      "\u001B[?25hCollecting textblob~=0.15.3\n",
      "  Using cached textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Collecting gensim~=4.1.2\n",
      "  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 24.1 MB 1.2 MB/s \n",
      "\u001B[?25hCollecting SPARQLWrapper~=1.8.5\n",
      "  Using cached SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
      "Collecting colorama~=0.4.4\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting networkx~=2.6.3\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting nest-asyncio~=1.5.5\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting PyYAML~=5.3.1\n",
      "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting pywsd~=1.2.4\n",
      "  Using cached pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
      "Collecting wn~=0.0.23\n",
      "  Using cached wn-0.0.23.tar.gz (31.6 MB)\n",
      "Collecting tqdm~=4.62.2\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting torch~=1.11.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 750.6 MB 12 kB/s \n",
      "\u001B[?25hCollecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting eventkit\n",
      "  Using cached eventkit-1.0.0-py3-none-any.whl (31 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "\u001B[K     |████████████████████████████████| 48 kB 5.8 MB/s \n",
      "\u001B[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.3 MB 78.0 MB/s \n",
      "\u001B[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001B[K     |████████████████████████████████| 772 kB 88.8 MB/s \n",
      "\u001B[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001B[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
      "\u001B[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001B[K     |████████████████████████████████| 880 kB 85.7 MB/s \n",
      "\u001B[?25hCollecting protobuf<=3.20.3,>=3.11.0\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.0 MB 70.3 MB/s \n",
      "\u001B[?25hCollecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting ujson\n",
      "  Downloading ujson-5.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001B[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
      "\u001B[?25hCollecting termcolor\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001B[K     |████████████████████████████████| 98 kB 9.3 MB/s \n",
      "\u001B[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.2 MB 91.0 MB/s \n",
      "\u001B[?25hCollecting python-dateutil>=2.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001B[K     |████████████████████████████████| 247 kB 99.7 MB/s \n",
      "\u001B[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001B[K     |████████████████████████████████| 297 kB 96.2 MB/s \n",
      "\u001B[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001B[K     |████████████████████████████████| 96 kB 6.2 MB/s \n",
      "\u001B[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001B[K     |████████████████████████████████| 181 kB 72.3 MB/s \n",
      "\u001B[?25hCollecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "\u001B[K     |████████████████████████████████| 671 kB 87.9 MB/s \n",
      "\u001B[?25hCollecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001B[K     |████████████████████████████████| 133 kB 57.0 MB/s \n",
      "\u001B[?25hCollecting typing-extensions<4.6.0,>=3.7.4.1\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001B[K     |████████████████████████████████| 130 kB 98.3 MB/s \n",
      "\u001B[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 13.7 MB 75.9 MB/s \n",
      "\u001B[?25hCollecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001B[K     |████████████████████████████████| 493 kB 40.2 MB/s \n",
      "\u001B[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001B[K     |████████████████████████████████| 48 kB 5.8 MB/s \n",
      "\u001B[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy~=3.2.1->clayrs) (45.2.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001B[K     |████████████████████████████████| 10.2 MB 62.5 MB/s \n",
      "\u001B[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001B[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
      "\u001B[?25hCollecting mysqlclient\n",
      "  Using cached mysqlclient-2.1.1.tar.gz (88 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001B[K     |████████████████████████████████| 502 kB 62.2 MB/s \n",
      "\u001B[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.3 MB 55.9 MB/s \n",
      "\u001B[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001B[K     |████████████████████████████████| 33.8 MB 55 kB/s \n",
      "\u001B[?25hCollecting rdflib>=4.0\n",
      "  Using cached rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "Collecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001B[K     |████████████████████████████████| 163 kB 75.1 MB/s \n",
      "\u001B[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001B[K     |████████████████████████████████| 61 kB 107 kB/s \n",
      "\u001B[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "\u001B[K     |████████████████████████████████| 123 kB 83.8 MB/s \n",
      "\u001B[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "\u001B[K     |████████████████████████████████| 195 kB 71.1 MB/s \n",
      "\u001B[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001B[K     |████████████████████████████████| 156 kB 77.7 MB/s \n",
      "\u001B[?25hCollecting wcwidth>=0.2.5\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.3 MB 70.7 MB/s \n",
      "\u001B[?25hCollecting isodate<0.7.0,>=0.6.0\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Building wheels for collected packages: clayrs, babelpy, sentence-transformers, PyYAML, wn, sacremoses, mysqlclient\n",
      "  Building wheel for clayrs (PEP 517) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for clayrs: filename=clayrs-0.4.0-py3-none-any.whl size=317885 sha256=abfcc6fe2c8863ba7601cf1236af42a493e3c029abd9418e11f72e2dd3f30b45\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/23/d3/56025a419c6d0344c2ece14362dff71825322329239ccfabdd\n",
      "  Building wheel for babelpy (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for babelpy: filename=BabelPy-1.0.1-py3-none-any.whl size=9722 sha256=40227f8e04a4535bda4e6f79c75a45474daa1044c2c4e29c6dd3588c03578a0c\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/8f/30/b724e0e8f88e79a32dc7358b52397421d71eac263bdb09e39a\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.1-py3-none-any.whl size=123300 sha256=263e1369c6194aee931e687a6f683ce8006a9e8cd28125df989e48ea42b3bcd3\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/e0/93/a0f6582848b87b07fa73c0f1fe2f43b6328fd3171fe852f381\n",
      "  Building wheel for PyYAML (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=44617 sha256=3051535c97c7f057f655eb191b6318a64135eda74f96a94a4cd6d31fe4d8719e\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c\n",
      "  Building wheel for wn (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792943 sha256=b9124dcc9e742a1fe7180a376f4b652b30de087743d46fb7ea207f76563b1f74\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/eb/fe/eb7c7be28c29ee90dd9d6f58c116673d0eb07b2d83dfb72a37\n",
      "  Building wheel for sacremoses (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895255 sha256=4c3c4b47e0f97cb0f5c894750b696b170415b59cad524bb65346558d011dfd05\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
      "  Building wheel for mysqlclient (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp38-cp38-linux_x86_64.whl size=109144 sha256=98c14ac3f000316060ad7d20ba1d2a6753b0f8cda0f76249db77426ad6e24646\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/e1/84/a6185eaec318899f59a32d393af7729a0719cd93695d71f9a1\n",
      "Successfully built clayrs babelpy sentence-transformers PyYAML wn sacremoses mysqlclient\n",
      "\u001B[31mERROR: torchvision 0.15.2 has requirement torch==2.0.1, but you'll have torch 1.11.0 which is incompatible.\u001B[0m\n",
      "Installing collected packages: dill, cloudpickle, numpy, eventkit, distex, fsspec, PyYAML, tqdm, filelock, typing-extensions, idna, urllib3, charset-normalizer, certifi, requests, packaging, huggingface-hub, tokenizers, regex, click, joblib, six, sacremoses, transformers, pyaml, protobuf, mysql-connector-python, wcwidth, ftfy, ujson, termcolor, nltk, pyparsing, kiwisolver, python-dateutil, cycler, matplotlib, colorama, ekphrasis, threadpoolctl, scipy, scikit-learn, cymem, langcodes, catalogue, murmurhash, preshed, blis, srsly, wasabi, pydantic, thinc, MarkupSafe, jinja2, typer, spacy-loggers, smart-open, pathy, spacy-legacy, spacy, babelpy, mysqlclient, mysql, pytz, pandas, whoosh, sentencepiece, torch, pillow, torchvision, sentence-transformers, textblob, gensim, isodate, rdflib, SPARQLWrapper, networkx, nest-asyncio, wn, pywsd, clayrs\n",
      "Successfully installed MarkupSafe-2.1.3 PyYAML-5.3.1 SPARQLWrapper-1.8.5 babelpy-1.0.1 blis-0.7.9 catalogue-2.0.8 certifi-2023.5.7 charset-normalizer-3.1.0 clayrs-0.4.0 click-8.1.3 cloudpickle-2.2.1 colorama-0.4.6 cycler-0.11.0 cymem-2.0.7 dill-0.3.6 distex-0.7.2 ekphrasis-0.5.4 eventkit-1.0.0 filelock-3.12.1 fsspec-2023.6.0 ftfy-6.1.1 gensim-4.1.2 huggingface-hub-0.15.1 idna-3.4 isodate-0.6.1 jinja2-3.1.2 joblib-1.2.0 kiwisolver-1.4.4 langcodes-3.3.0 matplotlib-3.2.2 murmurhash-1.0.9 mysql-0.0.3 mysql-connector-python-8.0.33 mysqlclient-2.1.1 nest-asyncio-1.5.6 networkx-2.6.3 nltk-3.8.1 numpy-1.21.6 packaging-23.1 pandas-1.2.5 pathy-0.10.1 pillow-9.5.0 preshed-3.0.8 protobuf-3.20.3 pyaml-21.10.1 pydantic-1.8.2 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2023.3 pywsd-1.2.5 rdflib-6.3.2 regex-2023.6.3 requests-2.31.0 sacremoses-0.0.53 scikit-learn-1.0.2 scipy-1.7.3 sentence-transformers-1.2.1 sentencepiece-0.1.99 six-1.16.0 smart-open-6.3.0 spacy-3.2.6 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 termcolor-2.3.0 textblob-0.15.3 thinc-8.0.17 threadpoolctl-3.1.0 tokenizers-0.10.3 torch-1.11.0 torchvision-0.15.2 tqdm-4.62.3 transformers-4.15.0 typer-0.4.2 typing-extensions-4.5.0 ujson-5.8.0 urllib3-2.0.3 wasabi-0.10.1 wcwidth-0.2.6 whoosh-2.7.4 wn-0.0.23\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "SPARQLWrapper",
         "_mysql_connector",
         "babelpy",
         "blis",
         "catalogue",
         "clayrs",
         "click",
         "cloudpickle",
         "cycler",
         "cymem",
         "dateutil",
         "dill",
         "distex",
         "ekphrasis",
         "eventkit",
         "google",
         "huggingface_hub",
         "jinja2",
         "joblib",
         "kiwisolver",
         "murmurhash",
         "mysql",
         "nest_asyncio",
         "networkx",
         "nltk",
         "preshed",
         "pyaml",
         "pydantic",
         "scipy",
         "sentence_transformers",
         "sentencepiece",
         "six",
         "sklearn",
         "smart_open",
         "spacy",
         "srsly",
         "textblob",
         "thinc",
         "torch",
         "tqdm",
         "ujson",
         "wasabi",
         "wcwidth",
         "whoosh",
         "yaml"
        ]
       }
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1MOZctwVyW5"
   },
   "source": [
    "# **! RESTART RUNTIME !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTZB50Y3rN-8"
   },
   "source": [
    "# Correct order log and prints for IPython\n",
    "This is necessary only for IPython environments (Colab, Jupyter, etc.), since they mess up the order of  ```print``` and ```logging```\n",
    "\n",
    "```python\n",
    "# EXAMPLE of the issue\n",
    ">>> import logging\n",
    ">>> print(\"Should go first\")\n",
    ">>> logging.warning(\"Should go second\")\n",
    "WARNING:root:Should go second\n",
    "Should go first\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GOTUezxnqzn7"
   },
   "source": [
    "import functools\n",
    "print = functools.partial(print, flush=True)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVKotTbvf_rC"
   },
   "source": [
    "# Import and datasets download\n",
    "\n",
    "The framework is made of three modules:\n",
    "> 1.   Content Analyzer\n",
    "> 2.   Recommender System\n",
    "> 3.   Evaluation\n",
    "\n",
    "We import every module as a library and use classes and methods by using the dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SctkIBio9dhe"
   },
   "source": [
    "from clayrs import content_analyzer as ca\n",
    "from clayrs import recsys as rs\n",
    "from clayrs import evaluation as eva\n",
    "\n",
    "# Usage:\n",
    "# ...\n",
    "# ca.Ratings()\n",
    "# rs.ContentBasedRS()\n",
    "# eva.EvalModel()\n",
    "# ..."
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYjcSfTtCXEw"
   },
   "source": [
    "We use **Movielens 100k** as dataset, with items info expanded thanks to imdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HwEnaPj4pvCS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aa82b124-3e2a-4896-c2d9-e8672b7b863f"
   },
   "source": [
    "# Dataset: Movielens-100k\n",
    "\n",
    "# download items_info\n",
    "!wget https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/items_info.json\n",
    "\n",
    "# download users_info\n",
    "!wget https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/users_info.csv\n",
    "\n",
    "# download ratings\n",
    "!wget https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/ratings.csv"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-06-11 18:38:25--  https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/items_info.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2222967 (2.1M) [text/plain]\n",
      "Saving to: ‘items_info.json’\n",
      "\n",
      "\ritems_info.json       0%[                    ]       0  --.-KB/s               \ritems_info.json     100%[===================>]   2.12M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-06-11 18:38:26 (161 MB/s) - ‘items_info.json’ saved [2222967/2222967]\n",
      "\n",
      "--2023-06-11 18:38:26--  https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/users_info.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22667 (22K) [text/plain]\n",
      "Saving to: ‘users_info.csv’\n",
      "\n",
      "users_info.csv      100%[===================>]  22.14K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-06-11 18:38:26 (65.2 MB/s) - ‘users_info.csv’ saved [22667/22667]\n",
      "\n",
      "--2023-06-11 18:38:26--  https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/ratings.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1979206 (1.9M) [text/plain]\n",
      "Saving to: ‘ratings.csv’\n",
      "\n",
      "ratings.csv         100%[===================>]   1.89M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-06-11 18:38:26 (88.8 MB/s) - ‘ratings.csv’ saved [1979206/1979206]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBOImtHzxTlQ"
   },
   "source": [
    "### Check items file\n",
    "In this example, the file containing items info is a JSON where every entry corresponds to a movie.\n",
    "\n",
    "For every movie there are various information, such as *genres, directors, cast, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAMUA6B6wSje",
    "outputId": "f2f4c487-3cc8-4185-ee97-906199092099"
   },
   "source": [
    "with open(\"items_info.json\", \"r\") as f:\n",
    "  # 25 lines but in these 23 lines there are only 2 entries:\n",
    "  # 'Toy Story', and 'Golden Eye'\n",
    "  for _ in range(25):\n",
    "    print(f.readline(), end='')\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "    {\n",
      "        \"movielens_id\": \"1\",\n",
      "        \"imdb_id\": \"0114709\",\n",
      "        \"title\": \"Toy Story\",\n",
      "        \"plot\": \"A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.\",\n",
      "        \"genres\": \"Animation, Adventure, Comedy, Family, Fantasy\",\n",
      "        \"year\": \"1995\",\n",
      "        \"rating\": \"8.3\",\n",
      "        \"directors\": \"John Lasseter\",\n",
      "        \"cast\": \"Tom Hanks, Tim Allen, Don Rickles, Jim Varney, Wallace Shawn, John Ratzenberger, Annie Potts, John Morris, Erik von Detten, Laurie Metcalf, R. Lee Ermey, Sarah Rayne, Penn Jillette, Jack Angel, Spencer Aste, Greg Berg, Lisa Bradley, Kendall Cunningham, Debi Derryberry, Cody Dorkin, Bill Farmer, Craig Good, Gregory Grudt, Danielle Judovits, Sam Lasseter, Brittany Levenbrown, Sherry Lynn, Scott McAfee, Mickie McGowan, Ryan O'Donohue, Jeff Pidgeon, Patrick Pinney, Phil Proctor, Jan Rabson, Joe Ranft, Andrew Stanton, Shane Sweet, Wayne Allwine, Tony Anselmo, Jonathan Benair, Anthony Burch, John Lasseter, Billy West\",\n",
      "        \"dbpedia_uri\": \"http://dbpedia.org/resource/Toy_Story\",\n",
      "        \"dbpedia_label\": \"Toy Story\"\n",
      "    },\n",
      "    {\n",
      "        \"movielens_id\": \"2\",\n",
      "        \"imdb_id\": \"0113189\",\n",
      "        \"title\": \"GoldenEye\",\n",
      "        \"plot\": \"Years after a friend and fellow 00 agent is killed on a joint mission, a secret space based weapons program known as \\\"_GoldenEye_ (qv)\\\" is stolen. James Bond sets out to stop a Russian crime syndicate from using the weapon.\",\n",
      "        \"genres\": \"Action, Adventure, Thriller\",\n",
      "        \"year\": \"1995\",\n",
      "        \"rating\": \"7.2\",\n",
      "        \"directors\": \"Martin Campbell\",\n",
      "        \"cast\": \"Pierce Brosnan, Sean Bean, Izabella Scorupco, Famke Janssen, Joe Don Baker, Judi Dench, Robbie Coltrane, Tchéky Karyo, Gottfried John, Alan Cumming, Desmond Llewelyn, Samantha Bond, Michael Kitchen, Serena Gordon, Simon Kunz, Pavel Douglas, Olivier Lajous, Billy J. Mitchell, Constantine Gregory, Minnie Driver, Michelle Arthur, Ravil Isyanov, Vladimir Milanovich, Trevor Byfield, Peter Majer, Paul Bannon, Simone Bechtel, Martin Campbell, Mark Chapman, Kenneth Coombs, Simon Crane, Terrance Denville, Ian Durrant, Max Faulkner, Juliet Forester, Stefan Kopiecki, Jo Anna Lee, Derek Lyons, Wayne Michaels, Bhasker Patel, Paul Sacks, Michael G. Wilson\",\n",
      "        \"dbpedia_uri\": \"http://dbpedia.org/resource/GoldenEye\",\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrGycV8NxkwF"
   },
   "source": [
    "### Check users file\n",
    "In this example, the file containing users info is a CSV file where the first column is the *user id*, while the other columns are side information for that user (*gender, occupation, zip code*)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUKLNAnMxEnh",
    "outputId": "ce342a33-703d-483f-99c8-f8229b37a0a0"
   },
   "source": [
    "with open(\"users_info.csv\", \"r\") as f:\n",
    "\n",
    "  # print the header and the first 2 entries\n",
    "  for _ in range(3):\n",
    "    print(f.readline(), end='')"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "user_id,age,gender,occupation,zip_code\n",
      "1,24,M,technician,85711\n",
      "2,53,F,other,94043\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dteEOqLxkDGR"
   },
   "source": [
    "<a name=\"cell-id\"></a>\n",
    "### Check ratings\n",
    "In this example, the file containing the interactions between the users and the movies is a CSV, where every interaction is a rating in the **[1, 5]** Likert scale"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Q3FvQsaIkbtz",
    "outputId": "21143799-c4f0-4d91-f816-96cfacb07d1d"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('ratings.csv')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       user_id  item_id  rating  timestamp\n",
       "0          196      242       3  881250949\n",
       "1          186      302       3  891717742\n",
       "2           22      377       1  878887116\n",
       "3          244       51       2  880606923\n",
       "4          166      346       1  886397596\n",
       "...        ...      ...     ...        ...\n",
       "99995      880      476       3  880175444\n",
       "99996      716      204       5  879795543\n",
       "99997      276     1090       1  874795795\n",
       "99998       13      225       2  882399156\n",
       "99999       12      203       3  879959583\n",
       "\n",
       "[100000 rows x 4 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-637d6260-21bc-4580-babe-ce5e68078cb6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>880</td>\n",
       "      <td>476</td>\n",
       "      <td>3</td>\n",
       "      <td>880175444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>716</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>879795543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>276</td>\n",
       "      <td>1090</td>\n",
       "      <td>1</td>\n",
       "      <td>874795795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>13</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>882399156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>879959583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-637d6260-21bc-4580-babe-ce5e68078cb6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-637d6260-21bc-4580-babe-ce5e68078cb6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-637d6260-21bc-4580-babe-ce5e68078cb6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilGpvNnPgHay"
   },
   "source": [
    "# Content Analyzer: representation of Items and export to json\n",
    "In order to define the *item representation*, the following parameters should be defined:\n",
    "*   ***source***: the path of the file containing items info\n",
    "*   ***id***: the field that uniquely identifies an item\n",
    "*   ***output_directory***: the path where serialized representations are saved\n",
    "\n",
    "There is also the optional parameter ***export_json***, which allows to create a file `contents.json` in the output directory containing the serialization of all representations of the content."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gORk6J3wqJGB"
   },
   "source": [
    "# Configuration of item representation \n",
    "movies_ca_config = ca.ItemAnalyzerConfig(\n",
    "    source=ca.JSONFile('items_info.json'),\n",
    "    id='movielens_id',\n",
    "    output_directory='movies_codified/',\n",
    "    export_json=True\n",
    ")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfnkSpfTjuDq"
   },
   "source": [
    "<a name=\"ca_id\"></a>\n",
    "Each item can be represented using a set of fields.\n",
    "Every field can be **represented** using several techniques, such as *'tfidf'*, *'entity linking'*, *'embeddings'*, etc.\n",
    "\n",
    "It is possible to process the content of each field using a **Natural Language Processing (NLP) pipeline**.  \n",
    "It is also possible to assign a **custom id** for each generated representation, in order to allow a simpler reference in the recommendation phase. Both NLP pipeline and custom id are optional parameters.\n",
    "\n",
    "> In the following example, we process the *'plot'* field by performing **lemmatization** and **stopwords removal** through [NLTK](https://www.nltk.org/), and we represent it using **tfidf**:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YA8GfrkrqJ4S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea93f3ea-ed81-4b2e-d128-a7589984b926"
   },
   "source": [
    "movies_ca_config.add_single_config(\n",
    "    'plot',\n",
    "    ca.FieldConfig(ca.SkLearnTfIdf(),\n",
    "                   preprocessing=ca.NLTK(stopwords_removal=True, lemmatization=True)) \n",
    ")"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAgjyRfVYRqG"
   },
   "source": [
    "In this example, we also add an exogenous representation to the content by extracting the 'year' field from the local source \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m9KBOFO-XAJ2"
   },
   "source": [
    "movies_ca_config.add_single_exogenous(\n",
    "    ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['year']))\n",
    ")"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e24qhQVkXIc"
   },
   "source": [
    "At the end of the configuration step, we provide the configuration to the *'Content Analyzer'* and call the `fit()` method:\n",
    "\n",
    "*   The Content Analyzer will **represent** and **serialize** every item (and create the json file containing representations for every content).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yXU-Vuw2twuN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2b8e43ae-4254-4012-a981-be40526ef61c"
   },
   "source": [
    "content_analyzer = ca.ContentAnalyzer(config=movies_ca_config)\n",
    "content_analyzer.fit()"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Extracting exogenous properties from local dataset\n",
      "\u001B[39mINFO\u001B[0m - ***********   Processing field: plot   ***********\n",
      "\u001B[39mINFO\u001B[0m - Computing tf-idf with SkLearnTfIdf\n",
      "Serializing contents:  100%|██████████| 1682/1682 [00:04<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrGYxny3g9sW"
   },
   "source": [
    "This is the JSON file which has been created:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfdoIwichaaF",
    "outputId": "22be94a1-0aac-4035-efa6-38e5e0e940e5"
   },
   "source": [
    "with open('movies_codified/contents.json', 'r') as f:\n",
    "    for _ in range(11):\n",
    "      print(f.readline(), end='')"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "    {\n",
      "        \"content_id\": \"1\",\n",
      "        \"Exo#0\": \"{'year': '1995'}\",\n",
      "        \"plot#0\": {\n",
      "            \"sparse_tfidf\": \"[[(0, 1024),0.1719365702112526],\\n [(0, 1729),0.31336848582007987],\\n [(0, 2160),0.3010691777558571],\\n [(0, 2721),0.25782007590969463],\\n [(0, 3752),0.2915290932564375],\\n [(0, 4769),0.1484604944785959],\\n [(0, 5439),0.31336848582007987],\\n [(0, 5932),0.2618948840931845],\\n [(0, 6417),0.3307033869191101],\\n [(0, 6655),0.3307033869191101],\\n [(0, 6853),0.25410006749357394],\\n [(0, 6909),0.2445599829941543],\\n [(0, 6941),0.31336848582007987]]\",\n",
      "            \"pos_word_tuples\": \"[(5932, 'room'), (1024, 'boy'), (6941, 'toy'), (6909, 'top'), (6655, 'supplants'), (2721, 'figure'), (6417, 'spaceman'), (4769, 'new'), (3752, 'jealous'), (6853, 'threaten'), (5439, 'profoundly'), (2160, 'doll'), (1729, 'cowboy')]\",\n",
      "            \"len_vocabulary\": 7614\n",
      "        }\n",
      "    },\n",
      "    {\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_3ff69ZhFj7"
   },
   "source": [
    "# [Optional] Content Analyzer: representation of Users and export to json\n",
    "In order to define the *'user representation'*, we could use the same process performed for *'item representation'*. In this case we don't want to represent in a complex way users, so this step is completely optional\n",
    "\n",
    "In this example, the ID for users is the column `user_id`.\n",
    "\n",
    "Also for users, it is ossible to export the representation in a ***json file***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HhwOj4s-uzOk"
   },
   "source": [
    "#Configuration of user representation\n",
    "users_ca_config = ca.UserAnalyzerConfig(\n",
    "    ca.CSVFile('users_info.csv'),\n",
    "    id='user_id',\n",
    "    output_directory='users_codified/',\n",
    "    export_json=True\n",
    ")"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8xsWBz8eYHF"
   },
   "source": [
    "We also add an exogenous representation for each user by extracting the 'gender' field from the local source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BELoRbCckKn",
    "outputId": "af1ed89c-b58d-484c-ed24-827c8a7d7055"
   },
   "source": [
    "users_ca_config.add_single_exogenous(\n",
    "    ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['gender']))\n",
    ")\n",
    "\n",
    "ca.ContentAnalyzer(config=users_ca_config).fit()"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Extracting exogenous properties from local dataset\n",
      "Serializing contents:  100%|██████████| 943/943 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnhE2zqnfykN"
   },
   "source": [
    "This is the JSON file which has been created:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8avOGAFvgGQy",
    "outputId": "9b500716-2187-45c2-8879-d6300ace60b8"
   },
   "source": [
    "with open('users_codified/contents.json', 'r') as f:\n",
    "    for _ in range(9):\n",
    "      print(f.readline(), end='')"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "    {\n",
      "        \"content_id\": \"1\",\n",
      "        \"Exo#0\": \"{'gender': 'M'}\"\n",
      "    },\n",
      "    {\n",
      "        \"content_id\": \"2\",\n",
      "        \"Exo#0\": \"{'gender': 'F'}\"\n",
      "    },\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr0qljcRhpW6"
   },
   "source": [
    "# Recommender System: centroid vector algorithm and export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms6kU0TEr_ps"
   },
   "source": [
    "The Recommender System module needs information about users, items and ratings. \n",
    "\n",
    "The **Ratings** class allows you to import rating from a source file (or also from an existent dataframe) into a custom object.   **If** the source file contains users (U), items (I) and ratings (R) in this order, no additional parameters are needed, **otherwise**  the mapping must be explictly specified using:\n",
    "\n",
    "*   **'user_id'** column,\n",
    "*   **'item_id'** column,\n",
    "*   **'score'** column\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjxZ9yTnvVW-",
    "outputId": "b7fba9e7-7d8b-44c0-b96c-5285e0e45399"
   },
   "source": [
    "ratings = ca.Ratings(ca.CSVFile('ratings.csv'))\n",
    "\n",
    "print(ratings)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Importing ratings:  100%|██████████| 100000/100000 [00:00<00:00]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user_id item_id  score\n",
      "0         196     242    3.0\n",
      "1         196     393    4.0\n",
      "2         196     381    4.0\n",
      "3         196     251    3.0\n",
      "4         196     655    5.0\n",
      "...       ...     ...    ...\n",
      "99995     941     919    5.0\n",
      "99996     941     273    3.0\n",
      "99997     941       1    5.0\n",
      "99998     941     294    4.0\n",
      "99999     941    1007    4.0\n",
      "\n",
      "[100000 rows x 3 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1SfXQx4y01u7"
   },
   "source": [
    "# (mapping by index) EQUIVALENT:\n",
    "#\n",
    "# ratings = ca.Ratings(\n",
    "#     ca.CSVFile('ratings.csv'),\n",
    "#     user_id_column=0,\n",
    "#     item_id_column=1,\n",
    "#     score_column=2\n",
    "# )"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ww9kKa3608pK"
   },
   "source": [
    "# (mapping by column name) EQUIVALENT:\n",
    "\n",
    "# ratings = ca.Ratings(\n",
    "#     ca.CSVFile('ratings.csv'),\n",
    "#     user_id_column='user_id',\n",
    "#     item_id_column='item_id',\n",
    "#     score_column='rating'\n",
    "# )"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9dYu-H9uUa5"
   },
   "source": [
    "The Recommender System also needs an algorithm for ranking or predicting items to users. In the following example we use the **CentroidVector** algorithm:\n",
    "\n",
    "*   It computes the centroid vector of the features of items *liked by the user*\n",
    "*   It computes the similarity between the centroid vector and unrated items\n",
    "\n",
    "The items liked by a user are those having a rating higher or equal than a specific **threshold**. If the threshold is not specified, the average score of all items liked by the user is used.\n",
    "\n",
    "The Recommender System leverages the representations defined by the Content Analyzer. In the current example, we use the representation of the field 'plot'. More representations could be adopted for a single field.\n",
    "\n",
    "\n",
    "```python\n",
    "# Example with multiple representations for a single field\n",
    "{\n",
    "  'plot': ['tfidf', 'word_embedding'],\n",
    "  'genre': 'doc_embedding',\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "Representations can be referenced using the **external id** (if specified, see [here](#ca_id)) or the **internal id**:\n",
    "\n",
    "\n",
    "```\n",
    "For the field 'plot':\n",
    "First representation created -> internal_id = 0\n",
    "Second representation created -> internal_id = 1\n",
    "...\n",
    "Nth representation created -> internal_id = n-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GbblAapgmOL6"
   },
   "source": [
    "centroid_vec = rs.CentroidVector(\n",
    "    {'plot': 0}, # the first and only representation codifed for the 'plot' field\n",
    "    similarity=rs.CosineSimilarity()\n",
    ")\n",
    "\n",
    "# no threshold parameter specified, the average rating given by\n",
    "# the user wil be used"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can instantiate the recommender system, we should perform the splitting of the dataset: let's perform a **KFold with 2 splits**\n",
    "\n",
    "*   The output of the partition module are two lists. One containing the two train set (in this case), the other containing the two test set (in this case)"
   ],
   "metadata": {
    "id": "un-SstzA4iEY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "kf = rs.KFoldPartitioning(n_splits=2, random_state=42)\n",
    "train_list, test_list = kf.split_all(ratings)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmTecQWq4iuN",
    "outputId": "fd6fe04d-94cf-4201-bbc0-5d4d183b5ac6"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Performing KFoldPartitioning:  100%|██████████| 943/943 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the ***cbrs*** must be fit before we can compute the rank:\n",
    "\n",
    "*   We could do this in two separate steps, by first calling the `fit(..)` method and then the `rank(...)` method \n",
    "\n",
    "*   Or by calling directly the `fit_rank(...)` method, which performs both in one step\n",
    "\n",
    "We use the second approach and we compute the rank for all users of our train set: we will use both the two train set and two test set obtained thanks to the KFold technique\n",
    "\n",
    "In order to compute a rank for all users, you simply do not specify the *user_id_list* parameter\n",
    "\n",
    "***Note:*** by default top-10 recommendations are returned for each user. In order to produce *unbounded ranking*, simply set `n_recs` parameter to `None`. In this case we are fine with the top-10 recs"
   ],
   "metadata": {
    "id": "ZNnqFrBZ4w7X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result_list = []\n",
    "\n",
    "for train_set, test_set in zip(train_list, test_list):\n",
    "  \n",
    "  cbrs = rs.ContentBasedRS(centroid_vec, train_set, 'movies_codified/')\n",
    "  rank_to_append = cbrs.fit_rank(test_set)\n",
    "\n",
    "  result_list.append(rank_to_append)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdMCfW1Z493a",
    "outputId": "7547e6d9-df04-4ae3-ce02-f18f780c256a"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Don't worry if it looks stuck at first\n",
      "\u001B[39mINFO\u001B[0m - First iterations will stabilize the estimated remaining time\n",
      "Computing fit_rank for user 492:  100%|██████████| 943/943 [00:24<00:00]\n",
      "\u001B[39mINFO\u001B[0m - Don't worry if it looks stuck at first\n",
      "\u001B[39mINFO\u001B[0m - First iterations will stabilize the estimated remaining time\n",
      "Computing fit_rank for user 492:  100%|██████████| 943/943 [00:28<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIQtO-Ah1Ty8"
   },
   "source": [
    "Let's export each ranking generated to a csv file\n",
    "\n",
    "*   The `rank()` method (and the `fit_rank()` method) returns a **Rank** object, that has a useful exporting method `to_csv()`\n",
    "\n",
    "We will save also the *test set* of each split, we need them later on in the *EvalModel part*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2z88KA1D1Xni"
   },
   "source": [
    "# we save the result of each split numbered\n",
    "for i, rank_generated in enumerate(result_list, start=1):\n",
    "  rank_generated.to_csv(file_name=f'rank_split_{i}')\n",
    "\n",
    "# we save  the result of each split numbered\n",
    "for i, test_set in enumerate(test_list, start=1):\n",
    "  test_set.to_csv(file_name=f'truth_split_{i}')"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IG3B5sUloNF"
   },
   "source": [
    "We can import recommendations we just exported via pandas, or any other library which reads csv file (also the framework itself):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qU72C15hl03T",
    "outputId": "bf9ecd7b-11f6-43ec-eca8-0dc66503d045"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "rank_split_1 = pd.read_csv('rank_split_1.csv')\n",
    "rank_split_2 = pd.read_csv('rank_split_2.csv')\n",
    "\n",
    "print(\"Result for first split:\")\n",
    "print(rank_split_1)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Result for second split:\")\n",
    "print(rank_split_2)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for first split:\n",
      "      user_id  item_id     score\n",
      "0         448      327  0.119112\n",
      "1         448     1602  0.060804\n",
      "2         448      340  0.058101\n",
      "3         448      268  0.054822\n",
      "4         448     1176  0.042615\n",
      "...       ...      ...       ...\n",
      "9425      492      286  0.079162\n",
      "9426      492      923  0.078905\n",
      "9427      492      528  0.062907\n",
      "9428      492      521  0.059115\n",
      "9429      492      275  0.052449\n",
      "\n",
      "[9430 rows x 3 columns]\n",
      "----------------------------------------\n",
      "Result for second split:\n",
      "      user_id  item_id     score\n",
      "0         448      262  0.053274\n",
      "1         448      900  0.041994\n",
      "2         448      319  0.040326\n",
      "3         448      288  0.030101\n",
      "4         448      345  0.027252\n",
      "...       ...      ...       ...\n",
      "9425      492      492  0.057705\n",
      "9426      492      212  0.055558\n",
      "9427      492      511  0.050925\n",
      "9428      492      699  0.042636\n",
      "9429      492     1098  0.037868\n",
      "\n",
      "[9430 rows x 3 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VlytYEsAEEh"
   },
   "source": [
    "# Evaluation module: evaluation of external recommendations\n",
    "\n",
    "Recommendations can be evaluated with several metrics using the **EvalModel** module of the framework. The nice part of it is that it can evaluate easily also (multiple) recommendations generated via external tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_oXZn3QAEEp"
   },
   "source": [
    "The Evaluation module needs the following parameters:\n",
    "\n",
    "*   A list of computed rank/predictions (in case multiple splits must be evaluated)\n",
    "*   A list of truths (in case multiple splits must be evaluated)\n",
    "*   List of metrics to compute\n",
    "\n",
    "Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position $i$ will be compared with the truth at position $i$\n",
    "\n",
    "Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the EvalModel class\n",
    "\n",
    "*   In this case we will use the recommendations generated earlier in the RecSys phase of this colab, but they are simple csv files and they could be the output of any other tool!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Importing split 1\")\n",
    "rank_1 = ca.Ratings(ca.CSVFile('rank_split_1.csv'))\n",
    "truth_1 = ca.Ratings(ca.CSVFile('truth_split_1.csv'))\n",
    "\n",
    "print(\"Importing split 2\")\n",
    "rank_2 = ca.Ratings(ca.CSVFile('rank_split_2.csv'))\n",
    "truth_2 = ca.Ratings(ca.CSVFile('truth_split_2.csv'))\n",
    "\n",
    "# since multiple splits, we wrap ranks and truths in lists\n",
    "imported_ranks = [rank_1, rank_2]\n",
    "imported_truths = [truth_1, truth_2]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRiA2LiiCRzU",
    "outputId": "56590c0d-a6ec-4ae5-acfe-1a27903123d1"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing split 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Importing ratings:  100%|██████████| 9430/9430 [00:00<00:00]\n",
      "Importing ratings:  100%|██████████| 50240/50240 [00:00<00:00]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing split 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Importing ratings:  100%|██████████| 9430/9430 [00:00<00:00]\n",
      "Importing ratings:  100%|██████████| 49760/49760 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are ready to instantiate the EvalModel class\n",
    "\n",
    "*   We also need to define a metric list, suppose we want to compute ***Pearson Correlation***, ***MRR*** and ***NDCG***\n",
    "\n"
   ],
   "metadata": {
    "id": "DQDcSQJmDxQM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SVtf_pFAAEEp"
   },
   "source": [
    "em = eva.EvalModel(\n",
    "    result_list,\n",
    "    test_list,\n",
    "    metric_list=[\n",
    "        eva.Correlation('pearson'),\n",
    "        eva.MRR(),\n",
    "        eva.NDCG()\n",
    "    ]\n",
    ")"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoFTI1T83TVV"
   },
   "source": [
    "The fit() method returns two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o9QXMFl02iDK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4a6077a8-0941-43a1-fbbe-5f685b20de37"
   },
   "source": [
    "sys_result, users_result =  em.fit()"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Performing evaluation on metrics chosen\n",
      "Performing NDCG:  100%|██████████| 3/3 [00:02<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the DataFrame which contains system results, the results are also grouped by splits"
   ],
   "metadata": {
    "id": "Fq8kTXb11SvG"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "yIpALBnr4Azi",
    "outputId": "96e5990e-4f1f-4946-bb80-fe3afdd3f9ba"
   },
   "source": [
    "sys_result"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              pearson       MRR      NDCG\n",
       "user_id                                  \n",
       "sys - fold1  0.034159  0.765408  0.921653\n",
       "sys - fold2  0.040059  0.785085  0.924107\n",
       "sys - mean   0.037109  0.775247  0.922880"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-136ed025-c7bb-4cff-b960-b30a392a290d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sys - fold1</th>\n",
       "      <td>0.034159</td>\n",
       "      <td>0.765408</td>\n",
       "      <td>0.921653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys - fold2</th>\n",
       "      <td>0.040059</td>\n",
       "      <td>0.785085</td>\n",
       "      <td>0.924107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys - mean</th>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.775247</td>\n",
       "      <td>0.922880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-136ed025-c7bb-4cff-b960-b30a392a290d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-136ed025-c7bb-4cff-b960-b30a392a290d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-136ed025-c7bb-4cff-b960-b30a392a290d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "R_-unvKt4BzC",
    "outputId": "ebcaa1d5-6e32-42b4-d53c-825251fc3de9"
   },
   "source": [
    "users_result"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          pearson      NDCG\n",
       "user_id                    \n",
       "1       -0.048485  0.867049\n",
       "10       0.242424  0.969331\n",
       "100     -0.333333  0.859255\n",
       "101      0.230303  0.912460\n",
       "102     -0.187879  0.861793\n",
       "...           ...       ...\n",
       "95      -0.121212  0.914743\n",
       "96      -0.012121  0.975030\n",
       "97       0.012121  0.952145\n",
       "98       0.333333  0.925285\n",
       "99      -0.109091  0.902449\n",
       "\n",
       "[943 rows x 2 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-6eecc135-f7c6-47ba-b4ad-440716661519\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.048485</td>\n",
       "      <td>0.867049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.969331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.859255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.912460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.187879</td>\n",
       "      <td>0.861793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.121212</td>\n",
       "      <td>0.914743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.012121</td>\n",
       "      <td>0.975030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.952145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.925285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.109091</td>\n",
       "      <td>0.902449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6eecc135-f7c6-47ba-b4ad-440716661519')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6eecc135-f7c6-47ba-b4ad-440716661519 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6eecc135-f7c6-47ba-b4ad-440716661519');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBeOgvjY2ECV"
   },
   "source": [
    "# Other modules: export splitted dataset and methodology module\n",
    "\n",
    "*    During recommendation phase we have already seen how to split the dataset. We will see a further example on how to split the dataset by considering only some users and how to export said splitted dataset.\n",
    "*    A peculiar parameter may be changed in the recommendation phase, the `methodology` parameter: it enables you to choose which items need to be predicted using a specific methodology. We'll see how to use it manually and how to export its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6rHhaW-AXN_"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDYYVUWM2hZi"
   },
   "source": [
    "In this example we split with a KFold all the users having a **mean average score >= 3.5**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "81yNWR0p2baT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0972cc0b-beed-49d4-d0c5-de7887353a27"
   },
   "source": [
    "import statistics\n",
    "\n",
    "# valid_users will contain all users that have a mean average score given >= 3.5\n",
    "valid_users = set()\n",
    "\n",
    "all_users = set(ratings.user_idx_column)\n",
    "for u in all_users:\n",
    "  user_ratings = ratings.get_user_interactions(u)\n",
    "  all_users_scores = user_ratings[:, 2]\n",
    "  if statistics.mean(all_users_scores) >= 3.5:\n",
    "    valid_users.add(u)\n",
    "\n",
    "\n",
    "# Print the length of the two sets to check that they are different\n",
    "print(f\"n_valid_users = {len(valid_users)}\")\n",
    "print(f\"n_all_users = {len(all_users)}\")"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_valid_users = 575\n",
      "n_all_users = 943\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SeA6PTI411J",
    "outputId": "060dba29-91d6-404a-9254-e8e7f6094585"
   },
   "source": [
    "part_technique = rs.KFoldPartitioning(n_splits=3, random_state=42)\n",
    "\n",
    "train_list, test_list = part_technique.split_all(ratings,\n",
    "                                                 user_id_list=valid_users)"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Performing KFoldPartitioning:  100%|██████████| 575/575 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puqxwcMo_41H"
   },
   "source": [
    "We can check the training and test set of the first split:  \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zxUsgcE_zGv",
    "outputId": "af929f16-581f-4a3d-f510-1318e0f708ff"
   },
   "source": [
    "first_train = train_list[0]\n",
    "first_test = test_list[0]\n",
    "\n",
    "print(\"First split - train set:\\n\")\n",
    "print(first_train)\n",
    "print(\"--------------------------------\")\n",
    "print(\"\\nFirst split - test set:\\n\")\n",
    "print(first_test)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First split - train set:\n",
      "\n",
      "      user_id item_id  score\n",
      "0          89     197    5.0\n",
      "1          89    1048    3.0\n",
      "2          89     732    5.0\n",
      "3          89     381    4.0\n",
      "4          89     127    5.0\n",
      "...       ...     ...    ...\n",
      "36817     492     172    3.0\n",
      "36818     492     699    3.0\n",
      "36819     492     474    5.0\n",
      "36820     492    1121    2.0\n",
      "36821     492    1098    4.0\n",
      "\n",
      "[36822 rows x 3 columns]\n",
      "--------------------------------\n",
      "\n",
      "First split - test set:\n",
      "\n",
      "      user_id item_id  score\n",
      "0          89     216    5.0\n",
      "1          89     246    5.0\n",
      "2          89     936    5.0\n",
      "3          89     221    1.0\n",
      "4          89     813    5.0\n",
      "...       ...     ...    ...\n",
      "18680     492     514    3.0\n",
      "18681     492     199    3.0\n",
      "18682     492     275    2.0\n",
      "18683     492     478    2.0\n",
      "18684     492     521    5.0\n",
      "\n",
      "[18685 rows x 3 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each object in the two lists is a **Rating** object, which has a useful exporting method `to_csv()`"
   ],
   "metadata": {
    "id": "_jmsk35NJWQ3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "first_train.to_csv(file_name='exported_split_train')\n",
    "first_test.to_csv(file_name='exported_split_test')"
   ],
   "metadata": {
    "id": "JHuPP2bxJjED"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st2i48dXBpZ3"
   },
   "source": [
    "## Methodology module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NknvogOFCLp7"
   },
   "source": [
    "During recommendation phase, it is also possible to specify an optional parameter in the `rank()` or `fit_rank()` method, called ***methodology***, for choosing which items must be ranked.\n",
    "For each target user **u**, the following 4 different methodologies are available for defining those lists:\n",
    "\n",
    "1.   **TestRatings** (default): the list of items to be evaluated consists of items rated by u in the test set\n",
    "2.   **TestItems**: every item in the test set of every user except those in the training set of the target user will be predicted\n",
    "3.   **TrainingItems**: every item in the training set of every user will be predicted except those in the training set of the target user\n",
    "4.   **AllItems**: the whole set of items, except those in the training set of the target user, will be predicted\n",
    "\n",
    "More information on [this paper](https://repositorio.uam.es/bitstream/handle/10486/665121/precision-oriented_bellogin_recsys_2011_ps.pdf;jsessionid=85982302D4DA9FF4DD7F21E4AC4F3391?sequence=1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have multiple splits you need to iterate over your train set and test set, since the `filter_all()` method of each methodology works on a single pair of train set and test set\n",
    "\n",
    "* You must call the `setup()` method before calling the `filter_all()` one!"
   ],
   "metadata": {
    "id": "xbBAXD5ZMVIp"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjtUGJoiEhpo",
    "outputId": "85b9fdb3-6c94-47e0-cb09-471543335de8"
   },
   "source": [
    "test_r = rs.TestRatingsMethodology().setup(first_train, first_test).filter_all(first_train, first_test)\n",
    "\n",
    "train_i = rs.TrainingItemsMethodology().setup(first_train, first_test).filter_all(first_train, first_test)"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Filtering items based on TestRatingsMethodology:  100%|██████████| 575/575 [00:00<00:00]\n",
      "Filtering items based on TrainingItemsMethodology:  100%|██████████| 575/575 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R1xu8mCF6sK"
   },
   "source": [
    "The output of the `filter_all()` method is a list of pandas DataFrame (one for every split in the split_list), so they can easily be exported to .csv, .tsv, etc.\n",
    "\n",
    "Let's check the item to predict with the test ratings methodology for the first split:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyGcZ-50Hgrj",
    "outputId": "78ae0036-5b53-499a-c049-8dfa9f9dff3c"
   },
   "source": [
    "print(test_r)"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user_id item_id\n",
      "0          89     880\n",
      "1          89     815\n",
      "2          89     213\n",
      "3          89      14\n",
      "4          89     221\n",
      "...       ...     ...\n",
      "18680     354     419\n",
      "18681     354     285\n",
      "18682     354     190\n",
      "18683     354     702\n",
      "18684     354     175\n",
      "\n",
      "[18685 rows x 2 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Report module"
   ],
   "metadata": {
    "id": "XHYkCpFUgBtZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Via the `Report` class, you can generate several ***yml*** files containing all the parameters passed to key classes and functions during your experiment.\n",
    "\n",
    "The mentioned class is very flexible and it is able to document various parts of the experiment, based on the module you use:\n",
    "\n",
    "* You can document how you preprocessed items and complexly represented them via the *Content Analyzer*\n",
    "* Or maybe the experimental setup of your recommendation pipeline by passing key objects of it to the Report class\n",
    "* And lastly how you evaluated your recommender systems and on which metrics\n",
    "\n",
    "The aim is to give you all the tools to ***reproduce*** the experiment also in a different experimental setup"
   ],
   "metadata": {
    "id": "thWR8EOugHZg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we import the utils package which contains the `Report` class"
   ],
   "metadata": {
    "id": "kWFlxu9jirze"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from clayrs import utils as ut"
   ],
   "metadata": {
    "id": "jDEHRrXAgDXU"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then you can instantiate the Report class. It needs the following parameters:\n",
    "* ***output_dir***: where the reports generated will be stored, by default it points to the current directory **('.')**\n",
    "* ***ca_report_filename***: how to rename the report of the content analyzer (if one is generated). Default is **'ca_report'**\n",
    "* ***rs_report_filename***: how to rename the report of the recsys module (if one is generated). Default is **'rs_report'**\n",
    "* ***eva_report_filename***: how to rename the report of the evaluation module (if one is generated). Default is **'eva_report'**"
   ],
   "metadata": {
    "id": "UQUiYnnjiyAq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# in this case we are satisfied with the default parameters\n",
    "rep = ut.Report()"
   ],
   "metadata": {
    "id": "X_Vnei_dixcb"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then simply call the *yaml()* method which will produce the three yaml reports.\n",
    "\n",
    "\n",
    "> You need to pass to it the objects instantiated and used to execute your experiment. For the recsys module and the evaluation module you first need to perform the actual experiment before you are able to obtain a report for them\n",
    "\n",
    "All the parameters of the function are *optional* so that you decide for which module a yaml report must be produced, in case you performed a partial experiment and only used *some* of the modules offered by the framework"
   ],
   "metadata": {
    "id": "KWsICicGkeVF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# In this case we generate a full report for the three modules used in the\n",
    "# experiment in this notebook\n",
    "rep.yaml(content_analyzer=content_analyzer,\n",
    "         original_ratings=ratings,\n",
    "         partitioning_technique=kf,\n",
    "         recsys=cbrs,\n",
    "         eval_model=em)"
   ],
   "metadata": {
    "id": "D-liANM5kTn_"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The three yml files are generated in the current directory"
   ],
   "metadata": {
    "id": "TA7sgavnlpum"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "assert os.path.isfile('ca_report.yml')\n",
    "assert os.path.isfile('rs_report.yml')\n",
    "assert os.path.isfile('eva_report.yml')"
   ],
   "metadata": {
    "id": "Ms8KI8WAlyNO"
   },
   "execution_count": 38,
   "outputs": []
  }
 ]
}
